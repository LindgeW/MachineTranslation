## Machine Translation
machine translation based on encoder-attention-decoder architecture.

<img src="imgs/nlm.jpg" style='align:center; width:60%;'>

### Dataset
https://nlp.stanford.edu/projects/nmt/

### Reference
[seq2seq](https://github.com/bentrevett/pytorch-seq2seq)

